{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.13 64-bit (windows store)' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'c:/Users/richa/AppData/Local/Microsoft/WindowsApps/python3.9.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "!pip install Scrapy\n",
    "!pip install Boto3\n",
    "!pip install psycopg2-binary\n",
    "import boto3\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import plotly.express as px\n",
    "import logging\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from sqlalchemy import create_engine, text , select, column\n",
    "\n",
    "list_of_cities =[\"Mont Saint Michel\",\n",
    "\"St Malo\",\n",
    "\"Bayeux\",\n",
    "\"Le Havre\",\n",
    "\"Rouen\",\n",
    "\"Paris\",\n",
    "\"Amiens\",\n",
    "\"Lille\",\n",
    "\"Strasbourg\",\n",
    "\"Chateau du Haut Koenigsbourg\",\n",
    "\"Colmar\",\n",
    "\"Eguisheim\",\n",
    "\"Besancon\",\n",
    "\"Dijon\",\n",
    "\"Annecy\",\n",
    "\"Grenoble\",\n",
    "\"Lyon\",\n",
    "\"Gorges du Verdon\",\n",
    "\"Bormes les Mimosas\",\n",
    "\"Cassis\",\n",
    "\"Marseille\",\n",
    "\"Aix en Provence\",\n",
    "\"Avignon\",\n",
    "\"Uzes\",\n",
    "\"Nimes\",\n",
    "\"Aigues Mortes\",\n",
    "\"Saintes Maries de la mer\",\n",
    "\"Collioure\",\n",
    "\"Carcassonne\",\n",
    "\"Ariege\",\n",
    "\"Toulouse\",\n",
    "\"Montauban\",\n",
    "\"Biarritz\",\n",
    "\"Bayonne\",\n",
    "\"La Rochelle\"]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "for i in list_of_cities:\n",
    "    # Make a request to the API to get the gps data for each city and put them in a df :\n",
    "    r = requests.get(\"https://nominatim.openstreetmap.org/search/{}?format=json&addressdetails=1&limit=1\".format(i))\n",
    "    data= data.append(r.json())\n",
    "    print(\"{}  done\".format(i))\n",
    "\n",
    "\n",
    "dataset_gps = data[['display_name','lon','lat']]\n",
    "#on va extraire le nom des villes et les ajouter dans une nouvelle colonne:\n",
    "nom_villes = pd.DataFrame(dataset_gps['display_name'].str.split(',',expand = True))\n",
    "nom_villes\n",
    "\n",
    "dataset_gps['nom_des_villes'] = nom_villes[0]\n",
    "dataset_gps = dataset_gps[['nom_des_villes','lon','lat']]\n",
    "dataset_gps\n",
    "#on prend un arrondi à 2 apres la virgule pour chercher les données météo ensuite des villes:\n",
    "dataset_gps['lon'] = dataset_gps['lon'].astype(float).round(2)\n",
    "dataset_gps['lat'] = dataset_gps['lat'].astype(float).round(2)\n",
    "\n",
    "\n",
    "#on veut récupérer les données météo pour chaque ville grâce à leurs coordonnées GPS:\n",
    "API_key = \"MyAPIKey\"\n",
    "\n",
    "#on crée un df vide pour y mettre les données météo de chaque ville:\n",
    "datameteo=pd.DataFrame()\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    lat = dataset_gps['lat'].values[i]\n",
    "    lon = dataset_gps['lon'].values[i]\n",
    "\n",
    "\n",
    "    url = ('https://api.openweathermap.org/data/2.5/onecall?lat={}&lon={}&units=metric&exclude=hourly,minutely&lang=fr&daily.rain&appid=MyAPIKey'.format(lat,lon))\n",
    "\n",
    "    response = requests.get(url)\n",
    "    datameteo = datameteo.append(response.json(), ignore_index=True)\n",
    "\n",
    "\n",
    "#on fusionne les df avec coordonnées gps et météo par ville:\n",
    "datamerge = datameteo.merge(dataset_gps)\n",
    "\n",
    "#On ajoute une colonne avec une  City Id unique(index + 1):\n",
    "#on réorganise l'ordre des colonnes:\n",
    "datamerge['City_Id']=datamerge.index + 1\n",
    "datamerge = datamerge[['City_Id','nom_des_villes','lat','lon','daily']]\n",
    "datamerge\n",
    "\n",
    "#on examine les données dans la colonne 'daily' du df:\n",
    "datamerge['daily'].values\n",
    "\n",
    "\n",
    "#on va mettre la colonne 'daily' sous forme de dictionnaire:\n",
    "dataset = {}\n",
    "for i in range(len(datamerge)):\n",
    "    dataset[i] = pd.DataFrame(datamerge['daily'].values[i])\n",
    "    print(dataset[i])\n",
    "    \n",
    "#on va voir la température pour la ville index 0 sur les 8 prochains jours:\n",
    "dataset[0]['temp']\n",
    "\n",
    "#on va voir la probabilité de pluie pour la ville index 0 sur les 8 prochains jours:\n",
    "dataset[0]['pop']\n",
    "\n",
    "#On va voir les taux d'humidité prévus pour la ville index 0 sur les 8 prochains jours:\n",
    "dataset[0]['humidity']\n",
    "\n",
    "\n",
    "#on fait des listes avec ces temps , pop et humidity:\n",
    "temp =[]\n",
    "pop=[]\n",
    "humidity=[]\n",
    "for i in range (0,35):\n",
    "    print('This is the expected temperatures over 8 days for city index {}'.format(i))\n",
    "    print(dataset[i]['temp'].values)\n",
    "    temp.append(dataset[i]['temp'].values)\n",
    "    print('This is the probability of precipitation for 8 days for city index {}'.format(i))\n",
    "    print(dataset[i]['pop'].values)\n",
    "    pop.append(dataset[i]['pop'].values)\n",
    "    print('The daily percentage of humidity for 8 days for city index {}'.format(i))\n",
    "    print(dataset[i]['humidity'].values)\n",
    "    humidity.append(dataset[i]['humidity'].values)\n",
    "\n",
    "#on fait une liste avec toutes les températures maximales par jour pour chaque ville:\n",
    "list_of_max_temp= []\n",
    "for i in range(0,35):\n",
    "    for j in range(0,8):\n",
    "        #print (np.array(temp)[i][j]['max'])\n",
    "        list_of_max_temp.append(np.array(temp)[i][j]['max'])\n",
    "list_of_max_temp\n",
    "\n",
    "#on fait une liste de températures max par jour sur 8 jours par ville:\n",
    "list_temp_max=[]\n",
    "for i in range(0,35):\n",
    "    i=i*8\n",
    "    list_temp_max.append(list_of_max_temp[i:i+8])\n",
    "list_temp_max\n",
    "\n",
    "\n",
    "#on va faire des listes des max temp,max pop et max humid par ville\n",
    "#on prend la valeur max dans chaque liste définies auparavant,\n",
    "# et ajouter à notre df en 3 nouvelles colonnes:\n",
    "\n",
    "listmaxtemp= []\n",
    "listmaxpop=[]\n",
    "listmaxhumid=[]\n",
    "maxtemp = pd.DataFrame()\n",
    "popmax = pd.DataFrame()\n",
    "humimax = pd.DataFrame()\n",
    "for i in range (0,35):\n",
    "    listmaxtemp.append(np.max(list_temp_max[i]))\n",
    "    listmaxpop.append(np.max(pop[i]))\n",
    "    listmaxhumid.append(np.max(humidity[i]))\n",
    "    maxtemp = pd.DataFrame(listmaxtemp)\n",
    "    popmax = pd.DataFrame(listmaxpop)\n",
    "    humimax = pd.DataFrame(listmaxhumid)\n",
    "    datamerge['temp_max']=maxtemp\n",
    "    datamerge['pop_max']=popmax\n",
    "    datamerge['humid_max']=humimax\n",
    "\n",
    "\n",
    "#on sauve le dataset obtenu en csv:\n",
    "datamerge.to_csv(r'C:\\Users\\richa\\Desktop\\Jedha\\Fullstack\\Projets\\Kayak\\DataCollection management')\n",
    "\n",
    "#on va faire une carte des températures maximales attendues dans les 8 prochains jours et faire varier la taille\n",
    "#des bulles selon l'humidité maximale attendue dans les 8 prochains jours:\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(datamerge, lat=\"lat\", lon=\"lon\",zoom=3, size ='humid_max',color = 'temp_max',\\\n",
    "                        mapbox_style=\"carto-positron\",hover_name =\"nom_des_villes\")\n",
    "fig.update_layout(title='Max temperature within the 8 next days')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#we are going to scrap, hotel's name,users score,city name,hotel description and hotel's urls,\n",
    "\n",
    "class BookingAllSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"bookingAll\"\n",
    "\n",
    "    # Starting URL\n",
    "    start_urls = ['https://www.booking.com/']\n",
    "\n",
    "    # Parse function for form request\n",
    "    def parse(self, response):\n",
    "        for city in list_of_cities:\n",
    "        \n",
    "            yield scrapy.FormRequest.from_response(               \n",
    "                response,\n",
    "                formdata={ \"ss\": city, \n",
    "                          \"data-checkin\":\"2022-06-06\",\n",
    "                          \"data-checkout\":\"2022-06-10\",\n",
    "                      \n",
    "                          },\n",
    "                callback=self.after_search\n",
    "            )\n",
    "\n",
    "    # Callback used after search\n",
    "    def after_search(self, response):\n",
    "        \n",
    "        \n",
    "        results = response.css('div.a1b3f50dcd.f7c6687c3d.a1f3ecff04.f996d8c258')\n",
    "                \n",
    "        for r in results:\n",
    "            yield {\n",
    "                'hotel_name': r.css('h3.a4225678b2 ::text').get(),\n",
    "                'url' : r.css('h3.a4225678b2 ::attr(href)').get(),\n",
    "                'city': r.css('span.f4bd0794db.b4273d69aa ::text').get(),\n",
    "                'Score given by the website users': r.css('div.b5cd09854e.d10a6220b4::text').get(),\n",
    "                'Text description of the hotel': r.css('div.d8eab2cf7f ::text').get(),\n",
    "            }\n",
    "\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"All_Booking.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "        }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "\n",
    "process.crawl(BookingAllSpider)\n",
    "process.start()\n",
    "\n",
    "#put the results in a df:\n",
    "a = pd.read_json('src/All_Booking.json')\n",
    "all_details = pd.DataFrame(a)\n",
    "all_details\n",
    "\n",
    "#we extract urls for each hotel in order to scrape the satellite data for each of them:\n",
    "list_of_urls = []\n",
    "for url in (all_details['url']):\n",
    "    list_of_urls.append(url)\n",
    "\n",
    "list_of_urls\n",
    "\n",
    "\n",
    "# we scrap the satellite data for each hotel:\n",
    "class BookingAllGeoSpider(scrapy.Spider):\n",
    "    # Name of your spider\n",
    "    name = \"allgeobooking\"\n",
    "    def start_requests(self):\n",
    "\n",
    "        # Starting URL\n",
    "        for url in list_of_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "            \n",
    "    def parse(self, response):\n",
    "        \n",
    "            results = response.css('#hotel_address') #we call the result by its unique #id\n",
    "            for r in results:\n",
    "                yield {                               \n",
    "                    'lat_lon': r.css('::attr(data-atlas-latlng)').get()}\n",
    "\n",
    "\n",
    "# Name of the file where the results will be saved\n",
    "filename = \"Allcoord.json\"\n",
    "\n",
    "# If file already exists, delete it before crawling (because Scrapy will concatenate the last and new results otherwise)\n",
    "if filename in os.listdir('src/'):\n",
    "        os.remove('src/' + filename)\n",
    "\n",
    "# Declare a new CrawlerProcess with some settings\n",
    "process = CrawlerProcess(settings = {\n",
    "    'USER_AGENT': 'Chrome/84.0 (compatible; MSIE 7.0; Windows NT 5.1)',\n",
    "    'LOG_LEVEL': logging.INFO,\n",
    "    \"FEEDS\": {\n",
    "        'src/' + filename : {\"format\": \"json\"},\n",
    "        }\n",
    "})\n",
    "\n",
    "# Start the crawling using the spider you defined above\n",
    "\n",
    "process.crawl(BookingAllGeoSpider)\n",
    "process.start()\n",
    "\n",
    "#we put the results in a df:\n",
    "allcoord = pd.DataFrame(b)\n",
    "allcoord\n",
    "\n",
    "#we split the lat and lon by ',' :\n",
    "allcoord = pd.DataFrame(allcoord['lat_lon'].str.split(',',expand = True))\n",
    "#we rename df columlns'name:\n",
    "allcoord['Lat_hotel'] = allcoord[0]\n",
    "allcoord['Long_hotel'] = allcoord[1]\n",
    "allcoord = allcoord[['Lat_hotel','Long_hotel']]\n",
    "allcoord\n",
    "\n",
    "#on fusionne les df avec les caractéristiques de chaque hotel avec le df de leurs coordonnées satelitte:\n",
    "data_all =all_details.join(allcoord)\n",
    "data_all\n",
    "\n",
    "# on remplace les notes manquantes par zéro et on arrondit les coordonnées à 5 chiffres apres la virgule:\n",
    "data_all['Score'] = data_all['Score given by the website users'].fillna(0)\n",
    "data_all['Long_hotel'] = data_all['Long_hotel'].astype(float).round(5)\n",
    "data_all['Lat_hotel'] = data_all['Lat_hotel'].astype(float).round(5)\n",
    "data_all = data_all[['city','hotel_name','url','Score','Lat_hotel','Long_hotel','Text description of the hotel']]\n",
    "data_all\n",
    "\n",
    "#on fait une carte avec tous les hôtels selon la notation donnée par les utilisateurs du site:\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(data_all,title ='Map with hotels scores given by the website users',lat=\"Lat_hotel\", lon=\"Long_hotel\",zoom=3, size ='Score', color = 'Score',color_continuous_scale='magma' ,mapbox_style=\"carto-positron\", hover_name=\"hotel_name\")\n",
    "fig.update_layout(\n",
    "    title_font_family=\"Times New Roman\",\n",
    "    title_font_color=\"black\",\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "#on sauve le dataset en csv\n",
    "data_all.to_csv(r'C:\\Users\\richa\\Desktop\\Jedha\\Fullstack\\Projets\\Kayak\\DataCollection management')\n",
    "\n",
    "\n",
    "#Partie S3/RDS:\n",
    "\n",
    "\n",
    "session = boto3.Session(aws_access_key_id=\"MyAWSAccessKey\", \n",
    "                        aws_secret_access_key=\"MyAWSSecretAccessKey\")\n",
    "\n",
    "\n",
    "#create a resource session\n",
    "s3 = session.resource(\"s3\")\n",
    "\n",
    "#create bucket\n",
    "bucket_name =s3.create_bucket(Bucket=\"kayak-details\")\n",
    "\n",
    "#we put the df in .csv format:\n",
    "data_all_hotels = data_all.to_csv()\n",
    "datamergecsv = datamerge.to_csv()\n",
    "\n",
    "#we put them in the s3 bucket created:\n",
    "put_object = bucket_name.put_object(Key=\"all-hotels-details.csv\", Body=data_all_hotels)\n",
    "\n",
    "put_object = bucket_name.put_object(Key=\"all-city-with-max-meteo.csv\", Body=datamergecsv)\n",
    "\n",
    "\n",
    "#on regarde à quoi ressemble le fichier avec les hotels les notes etc:\n",
    "data_meteo = pd.read_csv(r'C:\\Users\\richa\\Desktop\\Jedha\\Fullstack\\Projets\\Kayak')\n",
    "data_meteo\n",
    "\n",
    "#Create RDS server and database:\n",
    "\n",
    "engine = create_engine('postgresql+psycopg2://user:password@hostname:{port}/database_name')\n",
    "\n",
    "# connect the pandas dataframe with postgresql table\n",
    "data_meteo.to_sql('data_meteo', engine, if_exists='replace')\n",
    "\n",
    "\n",
    "#to get top 5 of the cities with higher temperature:\n",
    "pd.read_sql('SELECT \"Nom_des_villes\" FROM data_meteo ORDER BY temp_max DESC LIMIT 5', engine)\n",
    "\n",
    "\n",
    "# connect the pandas dataframe with postgresql table\n",
    "data_all.to_sql('data_all', engine, if_exists='replace')\n",
    "\n",
    "#on sélectionne les villes avec les noms des hotels, le score , et les coordonnées gps pour les mettre dans un df:\n",
    "dataframe = pd.read_sql('SELECT \"city\",hotel_name,\"Score\",\"Lat_hotel\",\"Long_hotel\" \\\n",
    "                FROM data_all', engine)\n",
    "\n",
    "#on sélectionne dans ce df les lignes qui contiennent le nom d'une des top 5 villes et on garde celles qui ont les top 20 score:\n",
    "df_Montauban = dataframe[dataframe['city'].str.contains('Montauban').values]\n",
    "df_Montauban = df_Montauban.nlargest(n=20, columns=['Score'], keep='all')\n",
    "df_Montauban\n",
    "\n",
    "df_Uzes = dataframe[dataframe['city'].str.contains('Uzès').values]\n",
    "df_Uzes = df_Uzes.nlargest(n=20, columns=['Score'], keep='all')\n",
    "df_Uzes\n",
    "\n",
    "df_Nîmes = dataframe[dataframe['city'].str.contains('Nîmes').values]\n",
    "df_Nîmes = df_Nîmes.nlargest(n=20, columns=['Score'], keep='all')\n",
    "df_Nîmes\n",
    "\n",
    "df_Avignon = dataframe[dataframe['city'].str.contains('Avignon').values]\n",
    "df_Avignon = df_Avignon.nlargest(n=20, columns=['Score'], keep='all')\n",
    "df_Avignon\n",
    "\n",
    "df_Grenoble = dataframe[dataframe['city'].str.contains('Grenoble').values]\n",
    "df_Grenoble = df_Grenoble.nlargest(n=20, columns=['Score'], keep='all')\n",
    "df_Grenoble\n",
    "\n",
    "#on combine les 5 datasets pour pouvoir ensuite faire la carte demandée:\n",
    "df_meteo_top5_city = pd.concat([df_Montauban,df_Uzes,df_Nîmes,df_Avignon,df_Grenoble], ignore_index=True)\n",
    "df_meteo_top5_city\n",
    "\n",
    "#we make a map with Top 20 hotels in the top 5 cities to go in France:\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(df_meteo_top5_city, lat=\"Lat_hotel\", lon=\"Long_hotel\",zoom=3, size ='Score',color = 'Score',\\\n",
    "                        mapbox_style=\"carto-positron\",hover_name =\"hotel_name\")\n",
    "fig.update_layout(title='Top 20 hotels for the top 5 cities to go in France')\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fd19e75c187e0ad7ca39ee06857227a43a194ec46474096fd211d01a3603487"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
